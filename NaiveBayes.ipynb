{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vocab as vocabulary\n",
    "import collections\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/data/ToxicityData/train.csv')\n",
    "test = pd.read_csv('/data/ToxicityData/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>Jeff Sessions is another one of Trump's Orwell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>I actually inspected the infrastructure on Gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>No it won't . That's just wishful thinking on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>Instead of wringing our hands and nibbling the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>how many of you commenters have garbage piled ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text\n",
       "0  7000000  Jeff Sessions is another one of Trump's Orwell...\n",
       "1  7000001  I actually inspected the infrastructure on Gra...\n",
       "2  7000002  No it won't . That's just wishful thinking on ...\n",
       "3  7000003  Instead of wringing our hands and nibbling the...\n",
       "4  7000004  how many of you commenters have garbage piled ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                 59848\n",
       "target                                                                                 0\n",
       "comment_text                           This is so cool. It's like, 'would you want yo...\n",
       "severe_toxicity                                                                        0\n",
       "obscene                                                                                0\n",
       "identity_attack                                                                        0\n",
       "insult                                                                                 0\n",
       "threat                                                                                 0\n",
       "asian                                                                                NaN\n",
       "atheist                                                                              NaN\n",
       "bisexual                                                                             NaN\n",
       "black                                                                                NaN\n",
       "buddhist                                                                             NaN\n",
       "christian                                                                            NaN\n",
       "female                                                                               NaN\n",
       "heterosexual                                                                         NaN\n",
       "hindu                                                                                NaN\n",
       "homosexual_gay_or_lesbian                                                            NaN\n",
       "intellectual_or_learning_disability                                                  NaN\n",
       "jewish                                                                               NaN\n",
       "latino                                                                               NaN\n",
       "male                                                                                 NaN\n",
       "muslim                                                                               NaN\n",
       "other_disability                                                                     NaN\n",
       "other_gender                                                                         NaN\n",
       "other_race_or_ethnicity                                                              NaN\n",
       "other_religion                                                                       NaN\n",
       "other_sexual_orientation                                                             NaN\n",
       "physical_disability                                                                  NaN\n",
       "psychiatric_or_mental_illness                                                        NaN\n",
       "transgender                                                                          NaN\n",
       "white                                                                                NaN\n",
       "created_date                                               2015-09-29 10:50:41.987077+00\n",
       "publication_id                                                                         2\n",
       "parent_id                                                                            NaN\n",
       "article_id                                                                          2006\n",
       "rating                                                                          rejected\n",
       "funny                                                                                  0\n",
       "wow                                                                                    0\n",
       "sad                                                                                    0\n",
       "likes                                                                                  0\n",
       "disagree                                                                               0\n",
       "sexual_explicit                                                                        0\n",
       "identity_annotator_count                                                               0\n",
       "toxicity_annotator_count                                                               4\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "This can be as simple as calling string.split() - good enough for English and many European languages - but we could also do something more sophisticated here. There are various types of tokenizers:  \n",
    "  \n",
    "1. nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "2. nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "white_token = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "keras_token = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 30000\n",
    "SEED = 23\n",
    "VAL_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first, tokenize everything to build vocab\n",
    "Only use vocabs from train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_all_one_list = white_token.tokenize(' '.join(train['comment_text'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1670966"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokenize_all_one_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1.67 million tokens, do not have to use all of them as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4261263),\n",
       " ('to', 2611234),\n",
       " ('and', 2096691),\n",
       " ('of', 2021781),\n",
       " ('a', 1880032),\n",
       " ('is', 1454734),\n",
       " ('in', 1294522),\n",
       " ('that', 1163635),\n",
       " ('for', 911179),\n",
       " ('I', 861783),\n",
       " ('you', 734810),\n",
       " ('are', 714218),\n",
       " ('be', 618319),\n",
       " ('not', 613791),\n",
       " ('have', 598834),\n",
       " ('it', 598509),\n",
       " ('on', 577471),\n",
       " ('with', 556536),\n",
       " ('as', 471924),\n",
       " ('they', 464629)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(tokenize_all_one_list).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('scant', 141)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(tokenize_all_one_list).most_common(V)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 30kth token has 140 appearances, not too bad, we will use top 30k covab, and leave the rest as unknown\n",
    "This step takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocabulary.Vocabulary(tokenize_all_one_list, size=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to IDs\n",
    "While there are a few ML models that operate directly on strings, in most cases (and always for neural networks) you'll need to convert the tokens to integer IDs that can index into a feature vector. To do this, we'll need to keep track of a vocabulary, which in its simplest form is just a dictionary.  \n",
    "\n",
    "And unlike before, we are now tokenizing every row and then turn them into IDs using our vocab created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [vocab.words_to_ids(white_token.tokenize(train_row)) for train_row in train['comment_text'].tolist()]\n",
    "x_test = [vocab.words_to_ids(white_token.tokenize(test_row)) for test_row in test['comment_text'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[82,\n",
       " 8,\n",
       " 52,\n",
       " 12488,\n",
       " 118,\n",
       " 3554,\n",
       " 2,\n",
       " 13,\n",
       " 102,\n",
       " 35,\n",
       " 1646,\n",
       " 4,\n",
       " 218,\n",
       " 2,\n",
       " 3494,\n",
       " 224,\n",
       " 4717,\n",
       " 164,\n",
       " 11879]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sparse input matrix\n",
    "For many language models, we need to convert inputs into a sparse matrix.   \n",
    "For example, for simple Naive Bayes BOW, we need to convert each sentence into an array containing the entire vocabulary. Each sentence would only have few words out of the entire vocab, so we will end up with a very sparse matrix, with each sentence being a row, and each row has V entries corresponding to the vocabulary.   \n",
    "  \n",
    "We have a function in utils to convert sentences into sparse matrix. In this representation, instead of printing V for each row, we only print the words that have a count > 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(train['target'] > 0.5)\n",
    "y_train = [1 if i else 0 for i in y_train]\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(SEED)\n",
    "rng.shuffle(x_train)  # in-place\n",
    "rng.shuffle(y_train)\n",
    "\n",
    "\n",
    "split_idx = int(VAL_SPLIT * len(x_train))\n",
    "val_x = x_train[:split_idx]\n",
    "val_y = y_train[:split_idx]\n",
    "\n",
    "train_x = x_train[split_idx:]\n",
    "train_y  = y_train[split_idx:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_sb = utils.id_lists_to_sparse_bow(train_x, V)\n",
    "val_x_sb = utils.id_lists_to_sparse_bow(val_x, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t15\n",
      "  (0, 3)\t5\n",
      "  (0, 4)\t6\n",
      "  (0, 5)\t2\n",
      "  (0, 7)\t2\n",
      "  (0, 8)\t4\n",
      "  (0, 9)\t6\n",
      "  (0, 10)\t2\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 14)\t2\n",
      "  (0, 18)\t3\n",
      "  (0, 20)\t3\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 26)\t2\n",
      "  (0, 30)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 35)\t2\n",
      "  (0, 36)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 43)\t3\n",
      "  (0, 47)\t1\n",
      "  (0, 51)\t1\n",
      "  (0, 55)\t1\n",
      "  :\t:\n",
      "  (0, 2392)\t1\n",
      "  (0, 3519)\t1\n",
      "  (0, 3643)\t1\n",
      "  (0, 3768)\t2\n",
      "  (0, 3839)\t1\n",
      "  (0, 3981)\t1\n",
      "  (0, 4346)\t1\n",
      "  (0, 4492)\t1\n",
      "  (0, 4668)\t1\n",
      "  (0, 4698)\t1\n",
      "  (0, 6773)\t1\n",
      "  (0, 8519)\t1\n",
      "  (0, 8528)\t1\n",
      "  (0, 9587)\t1\n",
      "  (0, 9857)\t2\n",
      "  (0, 10001)\t1\n",
      "  (0, 10309)\t4\n",
      "  (0, 10467)\t1\n",
      "  (0, 11152)\t2\n",
      "  (0, 16333)\t1\n",
      "  (0, 16840)\t1\n",
      "  (0, 17317)\t1\n",
      "  (0, 28213)\t1\n",
      "  (0, 28518)\t2\n",
      "  (0, 29701)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_x_sb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: x = (1263412, 30000) sparse, y = 1263412\n",
      "Test set:     x = (541462, 30000) sparse, y = 541462\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set: x = {:s} sparse, y = {:s}\".format(str(train_x_sb.shape), str(len(train_x))))\n",
    "print(\"Test set:     x = {:s} sparse, y = {:s}\".format(str(val_x_sb.shape),  str(len(val_x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "NB is used for classification, we we are going to turn the target variable into binary variable. \n",
    "This is only for testing purpose. For final model we do need a predicted probability so NB is out of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 93.94%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(train_x_sb, train_y)\n",
    "y_pred_val = nb.predict(val_x_sb)\n",
    "\n",
    "\n",
    "acc = accuracy_score(val_y, y_pred_val)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.94099549 0.04766949]\n",
      "recall: [0.99823561 0.00140898]\n",
      "fscore: [0.96877077 0.00273706]\n",
      "support: [509524  31938]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(val_y, y_pred_val)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'very', 'slippery', 'slope', 'here.', '--', 'The', 'popularity', 'of', '<unk>', 'oil', 'is', 'on', 'the', 'rise.', '<unk>', 'a', '<unk>', '<unk>', 'is', 'often', 'used', 'to', 'make', 'the', 'potent', 'marijuana', '<unk>', 'The', 'report', 'cites', 'a', 'recent', 'investigation', 'by', 'The', 'Oregonian', 'into', '<unk>', '<unk>', 'oil,', 'which', 'identified', 'nine', 'major', '<unk>', '<unk>', 'since', '2011,', 'including', 'one', 'that', 'killed', 'a', 'Portland', 'man.', '<unk>', 'THC', 'production', '-', '-', 'and', 'incidence', 'of', '<unk>', '<unk>', 'oil', 'lab', '<unk>', '--', 'is', 'expected', 'to', 'rise', 'as', 'the', 'market', 'expands', 'for', 'marijuana', '<unk>', 'and', 'demand', 'increases', 'for', 'product', 'that', 'has', 'a', 'strong', '<unk>', '<unk>', 'the', 'report', '<unk>', '<unk>']\n",
      "pred 0 actual 1\n",
      "['Because', 'the', 'Speaker', '<unk>', 'a', 'repeal', 'of', 'the', 'disaster', '<unk>', 'and', '<unk>', 'of', 'the', 'ACA,', 'you', 'claim', 'he', 'has', 'not', 'abandoned', 'his', '<unk>', '<unk>', 'Please.', 'Obamacare', 'was', 'every', 'bit', 'as', 'offensive', 'to', 'Catholic', 'social', 'teaching', 'as', 'uber', '<unk>']\n",
      "pred 0 actual 1\n",
      "['Trump', 'is', 'not', 'near', 'as', 'dangerous', 'as', 'the', 'Democrats', 'who', 'want', 'to', 'collude', 'with', 'foreign', 'powers.', '<unk>', '<unk>']\n",
      "pred 0 actual 1\n",
      "['I', 'remember', 'seeing', 'a', 'piece', 'of', 'the', 'Berlin', 'Wall', 'at', 'the', 'Reagan', 'Presidential', 'Library', 'accompanied', 'by', 'a', 'plaque', 'condemning', 'those', 'that', 'build', 'walls.']\n",
      "pred 0 actual 1\n",
      "['Someone', 'is', 'qualified', 'or', 'they', \"aren't.\", 'Dr.', 'Glass', 'has', 'a', '<unk>', 'two', '<unk>', '4', 'years', 'of', 'superintendent', 'experience,', 'and', 'was', '<unk>', 'Director', 'of', 'Ed', 'for', '<unk>', 'students.', '<unk>', 'had', '0', 'superintendent', 'experience,', 'no', '<unk>', '<unk>', 'and', \"hadn't\", 'run', 'a', 'large', 'organization.', \"It's\", 'disrespectful', 'to', 'Dr.', 'Glass', 'and', 'Eagle', 'County', 'SD', 'to', 'say', 'this', 'is', 'all', 'about', 'politics.', 'Dr.', 'Glass', 'still', 'has', 'the', 'full', 'support', 'and', 'gratitude', 'of', 'the', 'Eagle', 'School', 'Board.', 'Why?', \"It's\", 'because', 'he', 'puts', 'students', 'at', 'the', 'front', 'of', 'everything', 'he', 'has', 'done', 'there.', '\"We', 'have', 'been', 'fortunate', 'to', 'have', 'Dr.', 'Jason', 'Glass', 'as', 'our', 'leader', 'for', 'the', 'past', 'four', 'years.', 'Glass', 'has', 'exceeded', 'expectations', 'in', 'his', 'short', 'time', 'here,', 'and', 'his', 'shoes', 'will', 'be', 'hard', 'to', '<unk>', '<unk>', 'The', 'meeting', 'was', 'announced', 'when', '<unk>', 'named', 'Dr.', 'Glass', 'their', '<unk>', \"That's\", 'a', 'reasonable', '2', 'weeks', 'notice.', 'Everyone', 'should', 'receive', 'replies', 'from', 'the', '<unk>', 'If', 'someone', 'needs', 'help', 'because', 'they', \"haven't\", 'received', 'a', 'reply,', 'I', 'would', 'be', 'happy', 'to', 'look', 'into', 'it', 'for', 'them.']\n",
      "pred 0 actual 1\n",
      "['Have', 'you', 'ever', 'read', 'the', '<unk>', 'own', 'documents', 'on', 'cyclists', 'taking', '<unk>', '<unk>', '\"', 'Taking', 'a', 'lane', 'In', 'urban', 'areas', 'where', 'a', 'curb', 'lane', 'is', 'too', 'narrow', 'to', 'share', 'safely', 'with', 'a', '<unk>', 'it', 'is', 'legal', 'to', 'take', 'the', 'whole', 'lane', 'by', 'riding', 'in', 'the', 'centre', 'of', 'it.', 'On', '<unk>', 'roads,', 'it', 'is', 'not', 'safe', 'to', 'take', 'the', 'whole', 'lane.', 'To', 'move', 'left', 'in', 'a', 'lane,', 'shoulder', 'check,', 'signal', 'left', 'and', 'shoulder', 'check', 'again', 'then', 'move', 'to', 'the', 'centre', 'of', 'the', 'lane', 'when', 'it', 'is', 'safe', 'to', 'do', '<unk>']\n",
      "pred 0 actual 1\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for ind, val in enumerate(y_pred_val):\n",
    "    if val != val_y[ind]:\n",
    "        print(vocab.ids_to_words(val_x[ind]))\n",
    "        print(\"pred\", val, \"actual\", val_y[ind])\n",
    "        total += 1\n",
    "    if total > 5:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization\n",
    "Depending on the application, we might want to do some pre-processing to remove spurious variation in the text. For example, we might want to lowercase words to avoid storing separate features for \"I\" and \"i\", and we might want to replace numbers with a special token rather than keep track of every possible value.\n",
    "\n",
    "utils have a basic transformation in utils.canonicalize_word. It's important to write different one for different tasks since the use of language can be quite different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
